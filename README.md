This repository contains practical implementations of reinforcement learning algorithms, including Q-learning and Deep Q-Networks (DQN). In the GridWorld project, Q-learning was used to train an agent to navigate a 5x5 grid from the start at the top-left corner to the goal at the bottom-right, avoiding walls. The agent could move up, down, left, or right, and learned the best actions through trial and error over 1000 episodes, balancing exploration and exploitation. By the end of training, the agent reached the goal consistently and efficiently, demonstrating that it learned the optimal path through the grid.

In the CartPole-v1 project, a Deep Q-Network was used to train an agent to balance a pole on a moving cart by choosing left or right actions based on the environment’s state. The agent used a neural network to estimate action values, learned from past experiences through replay, and gradually shifted from exploring to exploiting the learned policy. After training, the agent was able to successfully balance the pole consistently, showing how DQN can solve dynamic control tasks.

The repository also includes a project on debugging Q-learning for the FrozenLake-v1 environment. The original code had issues such as incorrect Q-table setup, selecting the worst actions, missing learning rate, and no proper exploration strategy. After fixing these problems, the agent’s performance improved significantly, demonstrating the importance of correct implementation and balanced learning strategies in reinforcement learning.

Overall, this repository provides educational examples of how agents can learn to make decisions and optimize performance in different environments using both simple tabular methods and neural-network-based approaches.
